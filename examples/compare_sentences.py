"""
Experimento educacional para comparar padr√µes de self-attention
entre duas frases semanticamente diferentes.

Objetivos:
- Separar claramente duas m√©tricas:
    * aten√ß√£o recebida (attention_in)  -> import√¢ncia contextual
    * aten√ß√£o emitida (attention_out) -> atividade relacional
- Ver como pequenas mudan√ßas sem√¢nticas afetam a distribui√ß√£o de aten√ß√£o
- Trabalhar com uma √∫nica layer e head, de forma controlada

Este script √© explorat√≥rio e N√ÉO afirma causalidade nem "entendimento" do modelo.
"""

import sys
import os

# ------------------------------------------------------------
# Ajuste de path para permitir imports do diret√≥rio src
# ------------------------------------------------------------
ROOT_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..")
)
sys.path.append(ROOT_DIR)

# ------------------------------------------------------------
# Imports do projeto
# ------------------------------------------------------------
from src.model_loader import load_bert_pt
from src.attention_extractor import extract_attention
from src.comparison import compare_token_attention_simple
from src.visualization import plot_attention
from src.interactive_visualization import plot_attention_interactive

print("Iniciando experimento de compara√ß√£o de aten√ß√£o")

# ------------------------------------------------------------
# Frases escolhidas de forma controlada
# ------------------------------------------------------------
sentences = {
    "positiva": "O aluno passou na prova.",
    "negativa": "O aluno falhou na prova."
}

# ------------------------------------------------------------
# Carregando tokenizer e modelo BERT em portugu√™s
# ------------------------------------------------------------
tokenizer, model = load_bert_pt()

# ------------------------------------------------------------
# Extra√ß√£o de tokens e aten√ß√µes
# ------------------------------------------------------------
results = {}

for label, text in sentences.items():
    print(f"Extraindo aten√ß√£o da frase: {label}")
    data = extract_attention(text, tokenizer, model)
    results[label] = data

# ------------------------------------------------------------
# Layer e head escolhidos para an√°lise
# ------------------------------------------------------------
layer = 8
head = 3

# ------------------------------------------------------------
# Compara√ß√£o de m√©tricas de aten√ß√£o
# ------------------------------------------------------------
comparison = compare_token_attention_simple(
    attentions_a=results["positiva"]["attentions"],
    attentions_b=results["negativa"]["attentions"],
    tokens_a=results["positiva"]["tokens"],
    tokens_b=results["negativa"]["tokens"],
    layer=layer,
    head=head
)

# ------------------------------------------------------------
# RESULTADOS ‚Äî ATEN√á√ÉO RECEBIDA (IMPORT√ÇNCIA)
# ------------------------------------------------------------
print("\nüîπ Aten√ß√£o recebida ‚Äî frase positiva (import√¢ncia)")
for tok, score in comparison["ranking_in_a"]:
    print(f"{tok:>10} -> {score:.3f}")

print("\nüîπ Aten√ß√£o recebida ‚Äî frase negativa (import√¢ncia)")
for tok, score in comparison["ranking_in_b"]:
    print(f"{tok:>10} -> {score:.3f}")

print("\nüîπ Maior varia√ß√£o de aten√ß√£o recebida (positiva - negativa)")
for tok, diff in comparison["delta_in"]:
    print(f"{tok:>10} -> {diff:+.3f}")

# ------------------------------------------------------------
# RESULTADOS ‚Äî ATEN√á√ÉO EMITIDA (ATIVIDADE)
# ------------------------------------------------------------
print("\nüîπ Aten√ß√£o emitida ‚Äî frase positiva (atividade)")
for tok, score in comparison["attention_out_a"]:
    print(f"{tok:>10} -> {score:.3f}")

print("\nüîπ Aten√ß√£o emitida ‚Äî frase negativa (atividade)")
for tok, score in comparison["attention_out_b"]:
    print(f"{tok:>10} -> {score:.3f}")

# ------------------------------------------------------------
# Visualiza√ß√µes est√°ticas (Matplotlib)
# ------------------------------------------------------------
print("\nPlotando mapas de aten√ß√£o (Matplotlib)")

plot_attention(
    results["positiva"]["attentions"],
    results["positiva"]["tokens"],
    layer,
    head
)

plot_attention(
    results["negativa"]["attentions"],
    results["negativa"]["tokens"],
    layer,
    head
)

# ------------------------------------------------------------
# Visualiza√ß√µes interativas (Plotly)
# ------------------------------------------------------------
print("\nAbrindo visualiza√ß√µes interativas (Plotly)")

plot_attention_interactive(
    attentions=results["positiva"]["attentions"],
    tokens=results["positiva"]["tokens"],
    layer=layer,
    head=head,
    title_prefix="Aten√ß√£o ‚Äî Frase Positiva"
)

plot_attention_interactive(
    attentions=results["negativa"]["attentions"],
    tokens=results["negativa"]["tokens"],
    layer=layer,
    head=head,
    title_prefix="Aten√ß√£o ‚Äî Frase Negativa"
)
