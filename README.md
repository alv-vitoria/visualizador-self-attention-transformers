# ğŸ§  Visualizador de Self-Attention em transformers (Projeto Educacional)

<p align="center">
  <img src="assets/banner_attention.png" alt="Banner do Projeto" height="299px" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/PyTorch-2.x-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"/>
  <img src="https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black"/>
  <img src="https://img.shields.io/badge/Plotly-Interactive-3F4F75?style=for-the-badge&logo=plotly&logoColor=white"/>
  <img src="https://img.shields.io/badge/Matplotlib-Static%20Plots-11557C?style=for-the-badge&logo=plotly&logoColor=white"/>
  <img src="https://img.shields.io/badge/Seaborn-Statistical%20Viz-4C72B0?style=for-the-badge"/>
</p>

---

## VisÃ£o Geral

Este projeto educacional explora o mecanismo de **self-attention** em modelos
**Transformer**, com foco em **clareza conceitual**, **honestidade cientÃ­fica**
e **visualizaÃ§Ã£o didÃ¡tica**.

Utilizando modelos prÃ©-treinados da famÃ­lia **BERT**, o projeto permite:

- Extrair pesos de atenÃ§Ã£o de mÃºltiplas camadas (*layers*) e cabeÃ§as (*heads*)
- Visualizar mapas de atenÃ§Ã£o de forma **estÃ¡tica** e **interativa**
- Comparar padrÃµes de atenÃ§Ã£o entre frases semanticamente distintas
- Diferenciar explicitamente:
  - **atenÃ§Ã£o recebida** (*attention_in*) â†’ importÃ¢ncia contextual  
  - **atenÃ§Ã£o emitida** (*attention_out*) â†’ atividade relacional  

âš ï¸ **Importante**  
Este projeto **nÃ£o afirma** que atenÃ§Ã£o equivale a entendimento semÃ¢ntico, intencionalidade ou causalidade.  
As anÃ¡lises sÃ£o **exploratÃ³rias**, **heurÃ­sticas** e **educacionais**.

---

## Objetivos do Projeto

- Tornar o mecanismo de self-attention observÃ¡vel e inspecionÃ¡vel
- Demonstrar compreensÃ£o prÃ¡tica do funcionamento interno de Transformers
- Explorar como pequenas variaÃ§Ãµes textuais afetam padrÃµes de atenÃ§Ã£o
- Servir como material de estudo, aprendizado e portfÃ³lio tÃ©cnico
- Aplicar boas prÃ¡ticas de organizaÃ§Ã£o e modularizaÃ§Ã£o em Python

---
## Conceitos-Chave Trabalhados  
### AtenÃ§Ã£o recebida vs. atenÃ§Ã£o emitida   
Considere o tensor de atenÃ§Ã£o apÃ³s a aplicaÃ§Ã£o do *softmax*:    

A âˆˆ â„^(L Ã— H Ã— T Ã— T)

Onde:
- **L** â†’ nÃºmero de camadas (*layers*)
- **H** â†’ nÃºmero de cabeÃ§as de atenÃ§Ã£o (*heads*)
- **T** â†’ nÃºmero de tokens da sequÃªncia

O elemento **A [l, h, i, j]** representa quanto o token **i**
atribui atenÃ§Ã£o ao token **j**, na camada **l** e cabeÃ§a **h**.

Neste projeto, duas mÃ©tricas sÃ£o separadas explicitamente:

- **AtenÃ§Ã£o recebida (attention_in)**  
  Soma das colunas da matriz de atenÃ§Ã£o  
  â†’ *proxy de importÃ¢ncia contextual*

- **AtenÃ§Ã£o emitida (attention_out)**  
  Soma das linhas da matriz de atenÃ§Ã£o  
  â†’ *proxy de atividade relacional*

Essa separaÃ§Ã£o evita confusÃµes conceituais comuns ao interpretar mapas de atenÃ§Ã£o.

---

## ğŸ“ Estrutura do Projeto

```text
transformer_attention_tutorial/
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ step_by_step.py
â”‚   â””â”€â”€ compare_sentences.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ attention_extractor.py
â”‚   â”œâ”€â”€ attention_metrics.py
â”‚   â”œâ”€â”€ comparison.py
â”‚   â”œâ”€â”€ interactive_visualization.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ banner_attention.png
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Funcionalidades Implementadas

- ExtraÃ§Ã£o de attention weights por layer e head
- VisualizaÃ§Ã£o estÃ¡tica com Matplotlib e Seaborn
- VisualizaÃ§Ã£o interativa simples com Plotly
- ComparaÃ§Ã£o de padrÃµes de atenÃ§Ã£o entre frases
- Ranking automÃ¡tico de tokens por atenÃ§Ã£o recebida
- SeparaÃ§Ã£o clara entre mÃ©tricas de atenÃ§Ã£o
- Suporte a modelos em portuguÃªs e multilÃ­ngues

## VisualizaÃ§Ãµes | EstÃ¡tica
- Heatmaps tradicionais de atenÃ§Ã£o
- Ideal para documentaÃ§Ã£o e relatÃ³rios
- Baseada em Matplotlib

### VisualizaÃ§Ã£o Interativa (Opcional)

AlÃ©m das visualizaÃ§Ãµes estÃ¡ticas, o projeto inclui uma visualizaÃ§Ã£o interativa simples utilizando Plotly.

Essa visualizaÃ§Ã£o permite:
- Inspecionar valores individuais de atenÃ§Ã£o via mouse hover
- Aplicar zoom e navegar pelo mapa de atenÃ§Ã£o
- Explorar padrÃµes locais por layer e head
- Facilitar anÃ¡lises qualitativas sem alterar os cÃ¡lculos

#### *âš ï¸ ObservaÃ§Ã£o*    
A visualizaÃ§Ã£o interativa nÃ£o altera o significado matemÃ¡tico da atenÃ§Ã£o, nÃ£o adiciona novas mÃ©tricas, e nÃ£o substitui as figuras estÃ¡ticas.   
Ela apenas oferece outra forma de inspeÃ§Ã£o visual.

A implementaÃ§Ã£o com Plotly foi mantida intencionalmente simples, evitando dashboards ou aplicaÃ§Ãµes web complexas.

## Como Executar o Projeto
1ï¸âƒ£ Criar ambiente virtual
```
python -m venv venv
source venv/bin/activate    # Linux / Mac
venv\Scripts\activate       # Windows
```

2ï¸âƒ£ Instalar dependÃªncias
```
pip install torch transformers matplotlib seaborn plotly
```
3ï¸âƒ£ Executar exemplo guiado
```
python examples/step_by_step.py
```
4ï¸âƒ£ Executar comparaÃ§Ã£o entre frases
```
python examples/compare_sentences.py
```

## LimitaÃ§Ãµes Conhecidas

- AtenÃ§Ã£o nÃ£o representa causalidade;
- AtenÃ§Ã£o nÃ£o equivale a entendimento semÃ¢ntico;
- Resultados dependem da layer e head selecionadas;
- TokenizaÃ§Ã£o influencia comparaÃ§Ãµes diretas entre frases;  

Essas limitaÃ§Ãµes sÃ£o assumidas explicitamente.

## ğŸ“Œ Nota sobre o Escopo do Projeto

Este projeto nÃ£o Ã© um trabalho profissional de CiÃªncia de Dados ou Pesquisa em IA.

Projeto desenvolvido por uma profissional da Ã¡rea de suprimentos, com interesse em:

* Compreender como modelos Transformer distribuem atenÃ§Ã£o;   
* Observar pesos de atenÃ§Ã£o entre tokens;     
* Estudar fundamentos de NLP e deep learning de forma prÃ¡tica;        

Todas as anÃ¡lises tÃªm carÃ¡ter exploratÃ³rio, didÃ¡tico e nÃ£o causal.  
Nenhuma interpretaÃ§Ã£o cognitiva ou semÃ¢ntica Ã© assumida.

## âœ¨ Autora
VitÃ³ria Alvares dos Santos 
### Contatos:  
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/vitÃ³ria-alvares/)  
[![E-mail](https://img.shields.io/badge/-Email-000?style=for-the-badge&logo=microsoft-outlook&logoColor=007BFF)](mailto:Alvares26Sa@proton.me)  

### ConsideraÃ§Ãµes Finais   
*Este projeto foi desenvolvido com foco educacional,
priorizando clareza conceitual, honestidade metodolÃ³gica
e boas prÃ¡ticas de engenharia de software.*     
*Ele nÃ£o busca explicar o funcionamento interno completo de Transformers,
mas explorar os fundamentos da observaÃ§Ã£o crÃ­tica e aprendizado.*

> Nota: o arquivo `requirements.txt` foi gerado automaticamente a partir do ambiente virtual.  
> Ele inclui dependÃªncias diretas e indiretas necessÃ¡rias para execuÃ§Ã£o completa do projeto.

